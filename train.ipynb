{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mass includes\n",
    "import os, sys, warnings\n",
    "import math\n",
    "import torch as t\n",
    "import torchnet as tnt\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# add paths for all sub-folders\n",
    "paths = [root for root, _, _ in os.walk('.')\\\n",
    "         if 'evals' not in root]\n",
    "for item in paths:\n",
    "    sys.path.append(item)\n",
    "\n",
    "from ipynb.fs.full.config import Config\n",
    "from ipynb.fs.full.monitor import Visualizer\n",
    "from ipynb.fs.full.network import *\n",
    "from ipynb.fs.full.dataLoader import *\n",
    "from ipynb.fs.full.util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load config\n",
    "opt = Config()\n",
    "\n",
    "# setup random environment\n",
    "if opt.seed is None:\n",
    "    seed = t.seed()\n",
    "else:\n",
    "    seed = opt.seed\n",
    "t.manual_seed(seed)\n",
    "random.seed(seed % 2**32)\n",
    "np.random.seed(seed % 2**32)\n",
    "print('Use seed', seed)\n",
    "\n",
    "# define models\n",
    "net_enc = Encoder().to(opt.device)\n",
    "net_sum = Summarizer().to(opt.device)\n",
    "net_mi_disc = MIDiscriminator().to(opt.device)\n",
    "net_pr_disc = PirorDiscriminator().to(opt.device)\n",
    "\n",
    "# dataset for training\n",
    "train_dataset = ImageSet(opt,\n",
    "                         mode='train',\n",
    "                         norm=True,\n",
    "                         rand_trans=True,\n",
    "                         mask_out=True)\n",
    "train_loader = t.utils.data.DataLoader(train_dataset,\n",
    "                                       batch_size=opt.batch_size,\n",
    "                                       shuffle=True,\n",
    "                                       pin_memory=True,\n",
    "                                       num_workers=opt.num_workers,\n",
    "                                       drop_last=True)\n",
    "\n",
    "# dataset for testing\n",
    "test_dataset = ImageSet(opt,\n",
    "                        mode='test',\n",
    "                        norm=True,\n",
    "                        rand_trans=False,\n",
    "                        mask_out=True)\n",
    "test_loader = t.utils.data.DataLoader(test_dataset,\n",
    "                                      batch_size=opt.data_part[1])\n",
    "\n",
    "# optimizers\n",
    "enc_params = list(net_enc.parameters()) + list(net_sum.parameters())\n",
    "enc_optim = t.optim.Adam(enc_params, lr=opt.lr)\n",
    "disc_params = list(net_mi_disc.parameters()) + list(net_pr_disc.parameters())\n",
    "disc_optim = t.optim.Adam(disc_params, lr=opt.lr)\n",
    "\n",
    "# visualizer\n",
    "vis = Visualizer(env='InfoMax', port=8000)\n",
    "meter = tnt.meter.AverageValueMeter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    # set to evaluation mode\n",
    "    net_enc.eval()\n",
    "    net_sum.eval()\n",
    "    net_mi_disc.eval()\n",
    "    net_pr_disc.eval()\n",
    "\n",
    "    for s_idx, (sample, f_names) in enumerate(test_loader):\n",
    "        # copy to device\n",
    "        img = sample[:, :, :3, :, :].to(opt.device)\n",
    "        mask = sample[:, :, 3, :, :].to(opt.device).unsqueeze(2)\n",
    "\n",
    "        # reshape for batch processing\n",
    "        b, n, _, h, w = sample.size()\n",
    "        imgs = img.view(b * n, -1, h, w)\n",
    "        masks = mask.view(b * n, -1, h, w)\n",
    "\n",
    "        # central cropping\n",
    "        crop_h = math.floor(h / 2 - opt.crop_size / 2)\n",
    "        crop_w = math.floor(w / 2 - opt.crop_size / 2)\n",
    "        imgs = imgs[:, :, crop_h:crop_h + opt.crop_size,\n",
    "                    crop_w:crop_w + opt.crop_size]\n",
    "        masks = masks[:, :, crop_h:crop_h + opt.crop_size,\n",
    "                      crop_w:crop_w + opt.crop_size]\n",
    "\n",
    "        with t.no_grad():\n",
    "            # inference\n",
    "            pred_loc_feats, pred_loc_masks = net_enc(imgs, masks)\n",
    "            pred_glb_feats = net_sum(pred_loc_feats, pred_loc_masks)\n",
    "            disc_loc_feats, disc_glb_feats, disc_loc_masks = net_mi_disc(\n",
    "                pred_loc_feats, pred_glb_feats, pred_loc_masks)\n",
    "\n",
    "            # decouple batch\n",
    "            disc_loc_feats = disc_loc_feats.view(b, n, disc_loc_feats.size(1),\n",
    "                                                 disc_loc_feats.size(2))\n",
    "            disc_glb_feats = disc_glb_feats.view(b, n, disc_glb_feats.size(1),\n",
    "                                                 disc_glb_feats.size(2))\n",
    "            disc_loc_masks = disc_loc_masks.view(b, n, disc_loc_masks.size(1),\n",
    "                                                 disc_loc_masks.size(2))\n",
    "\n",
    "            # compute mutual information\n",
    "            mi_loss = calcDIMLoss(disc_loc_feats, disc_glb_feats,\n",
    "                                  disc_loc_masks)\n",
    "\n",
    "    # set to training mode\n",
    "    net_enc.train(mode=True)\n",
    "    net_sum.train(mode=True)\n",
    "    net_mi_disc.train(mode=True)\n",
    "    net_pr_disc.train(mode=True)\n",
    "\n",
    "    return mi_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training entry (visdom [link](http://localhost:8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_loss = 1e3\n",
    "for epoch in tqdm(range(opt.max_epoch), desc='epoch', total=opt.max_epoch):\n",
    "    # reset loss meter\n",
    "    meter.reset()\n",
    "\n",
    "    for index, (samples, _) in enumerate(train_loader):\n",
    "        # copy to device\n",
    "        imgs = samples[:, :, :3, :, :].to(opt.device)\n",
    "        masks = samples[:, :, 3, :, :].to(opt.device).unsqueeze(2)\n",
    "\n",
    "        # reshape for batch processing\n",
    "        b, n, _, h, w = samples.size()\n",
    "        imgs = imgs.view(b * n, -1, h, w)\n",
    "        masks = masks.view(b * n, -1, h, w)\n",
    "\n",
    "        # reset gradient\n",
    "        enc_optim.zero_grad()\n",
    "        disc_optim.zero_grad()\n",
    "\n",
    "        # extract local encodings\n",
    "        pred_loc_feats, pred_loc_masks = net_enc(imgs, masks)\n",
    "\n",
    "        # extract global encodings\n",
    "        pred_glb_feats = net_sum(pred_loc_feats, pred_loc_masks)\n",
    "\n",
    "        # compute mi discriminative encodings\n",
    "        disc_loc_feats, disc_glb_feats, disc_loc_masks = net_mi_disc(\n",
    "            pred_loc_feats, pred_glb_feats, pred_loc_masks)\n",
    "\n",
    "        # compute prior discriminative encodings\n",
    "        prior = t.rand_like(pred_glb_feats)\n",
    "        prior_disc = net_pr_disc(prior)\n",
    "        glb_disc = net_pr_disc(pred_glb_feats)\n",
    "\n",
    "        # decouple batch\n",
    "        disc_loc_feats = disc_loc_feats.view(b, n, disc_loc_feats.size(1),\n",
    "                                             disc_loc_feats.size(2))\n",
    "        disc_glb_feats = disc_glb_feats.view(b, n, disc_glb_feats.size(1),\n",
    "                                             disc_glb_feats.size(2))\n",
    "        disc_loc_masks = disc_loc_masks.view(b, n, disc_loc_masks.size(1),\n",
    "                                             disc_loc_masks.size(2))\n",
    "\n",
    "        # compute mutual information\n",
    "        mi_loss = calcDIMLoss(disc_loc_feats, disc_glb_feats, disc_loc_masks)\n",
    "\n",
    "        # compute cross entropy\n",
    "        pr_loss = -(t.log(prior_disc).mean() + t.log(1.0 - glb_disc).mean())\n",
    "\n",
    "        # update loss\n",
    "        loss = 1.0 * mi_loss + 0.1 * pr_loss\n",
    "\n",
    "        # update network params\n",
    "        loss.backward()\n",
    "        enc_optim.step()\n",
    "        disc_optim.step()\n",
    "\n",
    "        # add to loss meter for logging\n",
    "        meter.add(loss.item())\n",
    "\n",
    "    # save models if needed\n",
    "    val_loss = validate()\n",
    "    if val_loss < min_loss:\n",
    "        net_enc.save()\n",
    "        net_sum.save()\n",
    "        min_loss = val_loss\n",
    "\n",
    "    # show training status\n",
    "    if epoch + 1 >= 0.1 * opt.max_epoch:\n",
    "        vis.plot('training loss', meter.value()[0])\n",
    "\n",
    "    max_val = t.max(imgs[0, :, :, :])\n",
    "    min_val = t.min(imgs[0, :, :, :])\n",
    "    last_img = (imgs[0, :, :, :] - min_val) / (max_val - min_val)\n",
    "    vis.img('input', last_img)\n",
    "    vis.img('mask', masks[0, :, :, :])\n",
    "    vis.log('epoch: %d, cur val loss: %.4f, min val loss: %.4f' %\n",
    "            (epoch, val_loss, min_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
