{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mass includes\n",
    "import math\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from numpy.random import randint, uniform\n",
    "from torch.nn.functional import softplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     19,
     30
    ]
   },
   "outputs": [],
   "source": [
    "# random cropping\n",
    "def randCrop(in_img, crop_size):\n",
    "    # generate random coordinates\n",
    "    if isinstance(in_img, list):\n",
    "        _, hei, wid = in_img[0].shape\n",
    "    else:\n",
    "        _, hei, wid = in_img.shape\n",
    "    min_dim = min(crop_size, hei - 1, wid - 1)\n",
    "    crop_y = randint(hei - min_dim)\n",
    "    crop_x = randint(wid - min_dim)\n",
    "\n",
    "    # crop to given size\n",
    "    out_img = in_img[:, crop_y:crop_y + crop_size,\n",
    "                     crop_x:crop_x + crop_size].copy()\n",
    "\n",
    "    return out_img\n",
    "\n",
    "\n",
    "# random horizontal flipping\n",
    "def randHorFlip(in_img):\n",
    "    out_img = []\n",
    "    if uniform() > 0.5:\n",
    "        out_img = np.fliplr(in_img).copy()\n",
    "    else:\n",
    "        out_img = in_img\n",
    "\n",
    "    return out_img\n",
    "\n",
    "\n",
    "# random vertical flipping\n",
    "def randVerFlip(in_img):\n",
    "    out_img = []\n",
    "    if uniform() > 0.5:\n",
    "        out_img = np.flipud(in_img).copy()\n",
    "\n",
    "    else:\n",
    "        out_img = in_img\n",
    "\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     8,
     34,
     62,
     97
    ]
   },
   "outputs": [],
   "source": [
    "# code modified from https://github.com/rdevon/DIM\n",
    "def log_sum_exp(x, axis=None):\n",
    "    x_max = t.max(x, axis)[0]\n",
    "    y = t.log((t.exp(x - x_max)).sum(axis)) + x_max\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_positive_expectation(p_samples, measure, average=True):\n",
    "    log_2 = math.log(2.0)\n",
    "\n",
    "    if measure == 'GAN':\n",
    "        Ep = -softplus(-p_samples)\n",
    "    elif measure == 'JSD':\n",
    "        Ep = log_2 - softplus(-p_samples)  # Note JSD will be shifted\n",
    "    elif measure == 'X2':\n",
    "        Ep = p_samples**2\n",
    "    elif measure == 'KL':\n",
    "        Ep = p_samples\n",
    "    elif measure == 'RKL':\n",
    "        Ep = -t.exp(-p_samples)\n",
    "    elif measure == 'DV':\n",
    "        Ep = p_samples\n",
    "    elif measure == 'H2':\n",
    "        Ep = 1. - t.exp(-p_samples)\n",
    "    elif measure == 'W1':\n",
    "        Ep = p_samples\n",
    "\n",
    "    if average:\n",
    "        return Ep.mean()\n",
    "    else:\n",
    "        return Ep\n",
    "\n",
    "\n",
    "def get_negative_expectation(q_samples, measure, average=True):\n",
    "    log_2 = math.log(2.0)\n",
    "\n",
    "    if measure == 'GAN':\n",
    "        Eq = softplus(-q_samples) + q_samples\n",
    "    elif measure == 'JSD':\n",
    "        Eq = softplus(\n",
    "            -q_samples) + q_samples - log_2  # Note JSD will be shifted\n",
    "    elif measure == 'X2':\n",
    "        Eq = -0.5 * ((t.sqrt(q_samples**2) + 1.0)**2)\n",
    "    elif measure == 'KL':\n",
    "        Eq = t.exp(q_samples - 1.)\n",
    "    elif measure == 'RKL':\n",
    "        Eq = q_samples - 1.\n",
    "    elif measure == 'DV':\n",
    "        Eq = log_sum_exp(q_samples, 0) - math.log(q_samples.size(0))\n",
    "    elif measure == 'H2':\n",
    "        Eq = t.exp(q_samples) - 1.0\n",
    "    elif measure == 'W1':\n",
    "        Eq = q_samples\n",
    "\n",
    "    if average:\n",
    "        return Eq.mean()\n",
    "    else:\n",
    "        return Eq\n",
    "\n",
    "\n",
    "# f-divergence distance between positive and negative joint distributions\n",
    "def fenchel_dual_loss(l, m, measure=None):\n",
    "    assert measure in ['GAN', 'JSD', 'X2', 'KL', 'RKL', 'DV', 'H2',\n",
    "                       'W1'], print('Invalid measure: %s' % measure)\n",
    "\n",
    "    N, units, n_locals = l.size()\n",
    "\n",
    "    # First we make the input tensors the right shape.\n",
    "    l = l.view(N, units, n_locals)\n",
    "    l = l.permute(0, 2, 1)\n",
    "    l = l.reshape(-1, units)\n",
    "\n",
    "    m = m.view(N, units, 1)\n",
    "    m = m.permute(0, 2, 1)\n",
    "    m = m.reshape(-1, units)\n",
    "\n",
    "    # Outer product, we want a N x N x n_local x 1 tensor.\n",
    "    u = t.mm(m, l.t())\n",
    "    u = u.reshape(N, 1, N, n_locals).permute(0, 2, 3, 1)\n",
    "\n",
    "    # Since we have a big tensor with both positive and negative samples, we need to mask.\n",
    "    mask = t.eye(N).to(l.device)\n",
    "    n_mask = 1 - mask\n",
    "\n",
    "    # Compute the positive and negative score. Average the spatial locations.\n",
    "    E_pos = get_positive_expectation(u, measure, average=False).mean(2).mean(2)\n",
    "    E_neg = get_negative_expectation(u, measure, average=False).mean(2).mean(2)\n",
    "\n",
    "    # Mask positive and negative terms for positive and negative parts of loss\n",
    "    E_pos = (E_pos * mask).sum() / mask.sum()\n",
    "    E_neg = (E_neg * n_mask).sum() / n_mask.sum()\n",
    "    loss = E_neg - E_pos\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# loss function for deep infomax\n",
    "class DIMLoss(t.nn.Module):\n",
    "\n",
    "    def __init__(self, measure='JSD', scale=1.0, l2_penalty=0.0):\n",
    "        super(DIMLoss, self).__init__()\n",
    "        self.measure = measure\n",
    "        self.l2_penalty = l2_penalty\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, loc_feats, glb_feats):\n",
    "        loss = self.scale * fenchel_dual_loss(\n",
    "            loc_feats, glb_feats, measure=self.measure)\n",
    "\n",
    "        if self.l2_penalty > 0.:\n",
    "            loss = loss + self.l2_penalty * (glb_feats**2).sum(1).mean()\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
